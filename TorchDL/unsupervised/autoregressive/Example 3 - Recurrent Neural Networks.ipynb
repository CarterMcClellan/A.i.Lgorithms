{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivating Recurrent Neural Networks\n",
    "Picking up the discussion from where we left off, we proposed a model over 2 variables $x_1, x_2$, learning a distribution $p(x_1, x_2)$ as $p(x_1) \\cdot p(x_2 ~|~ x_1)$, where \n",
    "- $p(x_1)$ is estimated using a histogram\n",
    "- $p(x_2 ~|~ x_1)$ is estimated using a MLP\n",
    "\n",
    "The conclusion being that such a model is bad because\n",
    "- there is one function approximator per conditional\n",
    "- no information is shared amoungst different conditionals\n",
    "\n",
    "The solution to both problems being share parameters between a single function approximator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Review\n",
    "\n",
    "What is an RNN?\n",
    "```python\n",
    "def rnn_step(self, input, hidden):\n",
    "    # if we simply had\n",
    "    # hidden = torch.tanh(input @ self.Wxh + self.bh)\n",
    "    # it would just be a multi layer perceptron\n",
    "\n",
    "    # by adding hidden @ self.Whh we using the context of the previous\n",
    "    # hidden state\n",
    "    hidden = input @ self.Wxh +  hidden @ self.Whh + self.bh\n",
    "    output = hidden @ self.Why + self.by\n",
    "\n",
    "    output = self.softmax(output)\n",
    "\n",
    "    return output, hidden\n",
    "```\n",
    "The comment really covers it, an RNN distinguishes itself from a MLP in that with each step we are not only updating the weights of previous layers, but carrying forward the hidden state from past iterations. To see in more detail how the LSTM improves on the existing RNN model, see the nlp section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Torch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Torchvision Imports\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU acceleration\n",
    "gpu = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_SIZE = 28\n",
    "trans = transforms.Compose([\n",
    "                            transforms.Grayscale(), \n",
    "                            transforms.Resize((MNIST_SIZE, MNIST_SIZE)), \n",
    "                            transforms.ToTensor()\n",
    "])\n",
    "mnist_loader = torch.utils.data.DataLoader(datasets.MNIST(root='./data', train=True, download=True, \n",
    "                                                          transform=trans), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture Overview\n",
    "RNN combined with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Example(nn.Module):\n",
    "\n",
    "    def __init__(self, device, append_loc=False, input_shape=(1, 28, 28), hidden_size=256):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.append_loc = append_loc\n",
    "        self.input_channels = input_shape[0] + 2 if append_loc else input_shape[0]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_shape = input_shape\n",
    "        self.canvas_size = input_shape[1] * input_shape[2]\n",
    "\n",
    "        self.lstm = nn.LSTM(self.input_channels, self.hidden_size, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_size, input_shape[0])\n",
    "\n",
    "    def loss(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x_inp = append_location(x, self.device) if self.append_loc else x\n",
    "\n",
    "        # Shift input by one to the right\n",
    "        x_inp = x_inp.permute(0, 2, 3, 1).contiguous().view(batch_size, self.canvas_size, self.input_channels)\n",
    "        x_inp = torch.cat((torch.zeros(batch_size, 1, self.input_channels).to(self.device), x_inp[:, :-1]), dim=1)\n",
    "\n",
    "        h0 = torch.zeros(1, x_inp.size(0), self.hidden_size).to(self.device)\n",
    "        c0 = torch.zeros(1, x_inp.size(0), self.hidden_size).to(self.device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x_inp, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out).squeeze(-1) # b x 784\n",
    "\n",
    "        return F.binary_cross_entropy_with_logits(out, x.view(batch_size, -1))\n",
    "\n",
    "\n",
    "    def sample(self, n):\n",
    "        with torch.no_grad():\n",
    "            samples = torch.zeros(n, 1, self.input_channels).to(self.device)\n",
    "            h = torch.zeros(1, n, self.hidden_size).to(self.device)\n",
    "            c = torch.zeros(1, n, self.hidden_size).to(self.device)\n",
    "\n",
    "            for i in range(self.canvas_size):\n",
    "                x_inp = samples[:, [i]]\n",
    "                out, (h, c) = self.lstm(x_inp, (h, c))\n",
    "                out = self.fc(out[:, 0, :])\n",
    "                prob = torch.sigmoid(out)\n",
    "                sample_pixel = torch.bernoulli(prob).unsqueeze(-1) # n x 1 x 1\n",
    "                if self.append_loc:\n",
    "                    loc = np.array([i // 28, i % 28]) / 27\n",
    "                    loc = torch.FloatTensor(loc).to(self.device)\n",
    "                    loc = loc.view(1, 1, 2).repeat(n, 1, 1)\n",
    "                    sample_pixel = torch.cat((sample_pixel, loc), dim=-1)\n",
    "                samples = torch.cat((samples, sample_pixel), dim=1)\n",
    "\n",
    "            if self.append_loc:\n",
    "                samples = samples[:, 1:, 0] # only get sampled pixels, ignore location\n",
    "            else:\n",
    "                samples = samples[:, 1:].squeeze(-1) # n x 784\n",
    "            samples = samples.view(n, *self.input_shape)\n",
    "            return samples.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, d, num_epochs=100):\n",
    "    model = SimpleAutoregModel(d)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), desc='Training model'):  # loop over the dataset multiple times\n",
    "        for i, data in enumerate(train_loader):\n",
    "            x = data.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            loss = model.loss(x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
