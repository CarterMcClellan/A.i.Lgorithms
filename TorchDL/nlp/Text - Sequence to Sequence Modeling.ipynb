{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence 2 Sequence Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter as P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About\n",
    "Taking in a vector of variable length, and spitting out a vector of variable length. The most common real world example is Machine Translation.\n",
    "\n",
    "One of the key components is the LSTM cell which we built in the Text - Gated Neural Nets (LSTM, GRU) notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # input to hidden\n",
    "        self.Wxi = P(torch.randn(input_size, hidden_size)*.01)\n",
    "        self.Wxf = P(torch.randn(input_size, hidden_size)*.01)\n",
    "        self.Wxo = P(torch.randn(input_size, hidden_size)*.01)\n",
    "        self.Wxc = P(torch.randn(input_size, hidden_size)*.01)\n",
    "        \n",
    "        # hidden to hidden\n",
    "        self.Whi = P(torch.randn(hidden_size, hidden_size)*.01)\n",
    "        self.Whf = P(torch.randn(hidden_size, hidden_size)*.01)\n",
    "        self.Who = P(torch.randn(hidden_size, hidden_size)*.01)\n",
    "        self.Whc = P(torch.randn(hidden_size, hidden_size)*.01)\n",
    "        \n",
    "        # bias\n",
    "        self.bi = P(torch.zeros(1, hidden_size))\n",
    "        self.bf = P(torch.zeros(1, hidden_size))\n",
    "        self.bo = P(torch.zeros(1, hidden_size))\n",
    "        self.bc = P(torch.zeros(1, hidden_size))\n",
    "        \n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        h, c = hidden # previous h, c \n",
    "        \n",
    "        # sigmoid + linear map input, hidden -> hidden\n",
    "        i_t = torch.sigmoid(input @ self.Wxi + h @ self.Whi + self.bi)\n",
    "        f_t = torch.sigmoid(input @ self.Wxf + h @ self.Whf + self.bf)\n",
    "        o_t = torch.sigmoid(input @ self.Wxo + h @ self.Who + self.bo)\n",
    "        \n",
    "        # tanh + linear map input, hidden -> hidden\n",
    "        g_t = torch.tanh(input @ self.Wxc + h @ self.Whc + self.bc)\n",
    "        \n",
    "        # note that this is elementwise multiplication\n",
    "        # not matrix multiplication\n",
    "        c_t = c * f_t + i_t * g_t\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "        \n",
    "        return h_t, (h_t, c_t)\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size), torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Architecture\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # nn Embedding is just an array of tensors\n",
    "        # nn.Embedding(input_size, hidden_size) is an array\n",
    "        # holding (input size) tensors of size (hidden size)\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = LSTMCell(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Architecture\n",
    "The decoder is another RNN that takes the encoder output vector(s) and outputs a sequence of words to create the translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = LSTMCell(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
