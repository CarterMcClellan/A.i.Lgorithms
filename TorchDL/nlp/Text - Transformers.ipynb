{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Transformers\n",
    "There is some desire to allow for Networks to be trained in parallel, increasing the number of learnable parameters, this means taking advantage of gpus.\n",
    "\n",
    "But when you distribute training it can interfere with the attention mechanism. The ability to relate learned concepts in one network to learned concepts in another network will be ever increasing in complexity. To get around this, we want to use a Multi-Head attention mechanism."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
