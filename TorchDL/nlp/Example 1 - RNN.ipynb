{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Introduction - RNN\n",
    "\n",
    "#### References\n",
    "- [Pytorch RNN Introduction](!https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)\n",
    "- [Karpathy Blog](!http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/names/'\n",
    "\n",
    "# reproducability\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Data Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFiles(path): \n",
    "    return glob.glob(path)\n",
    "\n",
    "# print(findFiles(os.path.join(DATA_DIR, 'names/*.txt')))\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# print(unicodeToAscii('Ślusàrski'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles(os.path.join(DATA_DIR, 'names/*.txt')):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 letters, each vector has a single row, and 57 symbols torch.Size([5, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print('5 letters, each vector has a single row, and 57 symbols', lineToTensor('Jones').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TutRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # super so we inherit methods from nn.Module\n",
    "        super(TutRNN, self).__init__() \n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # stack tensors on top of each other\n",
    "        combined = torch.cat((input, hidden), 1) \n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # super so we inherit methods from nn.Module\n",
    "        super(ScratchRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # we want to initialize all these weights to something, \n",
    "        # otherwise when we run we will get a bunch of (nan)\n",
    "        self.Wxh = nn.Parameter(torch.randn(input_size, hidden_size)*0.01)\n",
    "    \n",
    "        self.Whh = nn.Parameter(torch.randn(hidden_size, hidden_size)*0.01)\n",
    "        self.bh = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "        \n",
    "        self.Why = nn.Parameter(torch.randn(hidden_size, output_size)*0.01)\n",
    "        self.by = nn.Parameter(torch.zeros(1, output_size))\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "            \n",
    "    def forward(self, input, hidden):\n",
    "        # if we simply had\n",
    "        # hidden = torch.tanh(input @ self.Wxh + self.bh)\n",
    "        # it would just be a multi layer perceptron\n",
    "        \n",
    "        # by adding hidden @ self.Whh we using the context of the previous\n",
    "        # hidden state\n",
    "        hidden = input @ self.Wxh +  hidden @ self.Whh + self.bh\n",
    "        output = hidden @ self.Why + self.by\n",
    "        \n",
    "        output = self.softmax(output)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss() # this is somehow appropriate to go with Log Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Note\n",
    "Note that we put all the words through to get a finalized hidden state before we do our little backprop procedure. In other words we account for variable length.\n",
    "\n",
    "Note too that after some point the hidden state is being set back to 0 again. This variable exists on a per input basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(rnn, category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    # this is doing what an optimizer does\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just return an output given a line\n",
    "def evaluate(rnn, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Note 2\n",
    "- GPU acceleration on a RNN is much less than that of a CNN likely because things like operations like conv2d are much more paralleisable then a feedforward simple linear net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05555555555555555"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/n_categories # we should be way higher than this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rnn1 (Rnn Built in Tutorial Using Torch Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 10000\n",
    "epochs = 2\n",
    "n_hidden = 128\n",
    "rnn = TutRNN(n_letters, n_hidden, n_categories)\n",
    "\n",
    "# rnn.to(device)\n",
    "for ep in range(epochs):\n",
    "    for mode in [\"train\", \"val\"]:\n",
    "        num_correct = 0\n",
    "        for iter in range(n_iters):\n",
    "            category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "            if mode == \"train\":\n",
    "                output, loss = train(rnn, category_tensor, line_tensor)\n",
    "            elif mode == \"val\":\n",
    "                output = evaluate(rnn, line_tensor)\n",
    "                guess, guess_i = categoryFromOutput(output)\n",
    "                if guess == category:\n",
    "                    num_correct += 1\n",
    "    print(\"Percent Correct {} on Epoch {}\".format(num_correct/n_iters, ep+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rnn2 (Rnn built using matrix multiplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Correct 0.21 on Epoch 1\n",
      "Percent Correct 0.2396 on Epoch 2\n"
     ]
    }
   ],
   "source": [
    "n_iters = 10000\n",
    "epochs = 2\n",
    "n_hidden = 128\n",
    "rnn = ScratchRNN(n_letters, n_hidden, n_categories)\n",
    "\n",
    "# rnn.to(device) \n",
    "\n",
    "# declaring a variable as cuda changes the computational graph\n",
    "# in such a way that this form of optimization is does not work\n",
    "# therefore we dont use rnn.to(cuda)\n",
    "for ep in range(epochs):\n",
    "    for mode in [\"train\", \"val\"]:\n",
    "        num_correct = 0\n",
    "        for iter in range(n_iters):\n",
    "            category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "            if mode == \"train\":\n",
    "                output, loss = train(rnn, category_tensor, line_tensor)\n",
    "            elif mode == \"val\":\n",
    "                output = evaluate(rnn, line_tensor)\n",
    "                guess, guess_i = categoryFromOutput(output)\n",
    "                if guess == category:\n",
    "                    num_correct += 1\n",
    "    print(\"Percent Correct {} on Epoch {}\".format(num_correct/n_iters, ep+1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
