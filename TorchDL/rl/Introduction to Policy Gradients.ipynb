{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO\n",
    "- Write out training video to visualize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Methods\n",
    "Starting with the basics, we have some policy $\\pi_\\theta$, and some return $R$. The goal of our reinforcement learning algorithm is to maximize the expected return of the policy, which for convience sake is denoted as $J(\\pi_\\theta)$\n",
    "$$E[R(\\pi_\\theta)] = J(\\pi_\\theta)$$\n",
    "One way to optimize this policy would be to use gradient descent, for example\n",
    "$$\\theta_{k+1} = \\theta_k + \\alpha \\cdot \\nabla J(\\pi_\\theta)$$\n",
    "\n",
    "Note that the big thing here is finding $\\nabla J(\\pi_\\theta)$, and the algorithms that do this are called *policy gradient algorithms*\n",
    "\n",
    "**Derivation** breaks up into 2 parts\n",
    "- First we figure out what the gradient is supposed to be (mathematically)\n",
    "- Second we go about building a model to estimate that gradient as efficiently as possible\n",
    "\n",
    "### Part 1- Finding the Gradient\n",
    "$$ \\nabla J(\\pi_\\theta) = \\nabla E[R(\\pi_\\theta)] = \\nabla \\int_\\tau P(\\tau ~|~ \\theta) R(\\tau) = \\int_\\tau \\nabla P(\\tau ~|~ \\theta) R(\\tau)$$\n",
    "$$\\int_\\tau \\nabla P(\\tau ~|~ \\theta) R(\\tau) = \\int_\\tau P(\\tau ~|~ \\theta) \\nabla \\log P(\\tau ~|~ \\theta) R(\\tau) = E[\\nabla \\log P(\\tau ~|~ \\theta) R(\\tau)] = E\\left[\\sum \\log \\pi(a_t ~|~ s_t) R(\\tau)\\right]$$\n",
    "[more details](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient)\n",
    "\n",
    "### Part 2 - Building the Algorithm\n",
    "Now that we know\n",
    "$$\\nabla J(\\pi_\\theta) =  E\\left[\\sum \\log \\pi(a_t ~|~ s_t) R(\\tau)\\right]$$\n",
    "This expression is now written in such a way we can approximate the expectation or mean, by sampling large samples\n",
    "$$E\\left[\\sum \\log \\pi(a_t ~|~ s_t) R(\\tau)\\right] \\approx \\frac{1}{|D|} \\sum_{\\tau \\in D} \\sum_{t=0}^T \\nabla \\log \\pi_\\theta (a_t~|~s_t)R(\\tau) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(sizes, activation=nn.Tanh, output_activation=nn.Identity):\n",
    "    # Build a feedforward neural network.\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env_name='CartPole-v0', hidden_sizes=[32], lr=1e-2, \n",
    "          epochs=50, batch_size=5000, render=False):\n",
    "\n",
    "    # make environment, check spaces, get obs / act dims\n",
    "    env = gym.make(env_name)\n",
    "    assert isinstance(env.observation_space, Box), \\\n",
    "        \"This example only works for envs with continuous state spaces.\"\n",
    "    assert isinstance(env.action_space, Discrete), \\\n",
    "        \"This example only works for envs with discrete action spaces.\"\n",
    "\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    n_acts = env.action_space.n\n",
    "\n",
    "    # make core of policy network\n",
    "    logits_net = mlp(sizes=[obs_dim]+hidden_sizes+[n_acts])\n",
    "\n",
    "    # make function to compute action distribution\n",
    "    def get_policy(obs):\n",
    "        logits = logits_net(obs)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    # make action selection function (outputs int actions, sampled from policy)\n",
    "    def get_action(obs):\n",
    "        return get_policy(obs).sample().item()\n",
    "\n",
    "    # make loss function whose gradient, for the right data, is policy gradient\n",
    "    def compute_loss(obs, act, weights):\n",
    "        logp = get_policy(obs).log_prob(act)\n",
    "        return -(logp * weights).mean()\n",
    "\n",
    "    # make optimizer\n",
    "    optimizer = Adam(logits_net.parameters(), lr=lr)\n",
    "\n",
    "    # for training policy\n",
    "    def train_one_epoch():\n",
    "        # make some empty lists for logging.\n",
    "        batch_obs = []          # for observations\n",
    "        batch_acts = []         # for actions\n",
    "        batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "        batch_rets = []         # for measuring episode returns\n",
    "        batch_lens = []         # for measuring episode lengths\n",
    "\n",
    "        # reset episode-specific variables\n",
    "        obs = env.reset()       # first obs comes from starting distribution\n",
    "        done = False            # signal from environment that episode is over\n",
    "        ep_rews = []            # list for rewards accrued throughout ep\n",
    "\n",
    "        # render first episode of each epoch\n",
    "        finished_rendering_this_epoch = False\n",
    "\n",
    "        # collect experience by acting in the environment with current policy\n",
    "        while True:\n",
    "\n",
    "            # rendering\n",
    "            if (not finished_rendering_this_epoch) and render:\n",
    "                env.render()\n",
    "\n",
    "            # save obs\n",
    "            batch_obs.append(obs.copy())\n",
    "\n",
    "            # act in the environment\n",
    "            act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "            obs, rew, done, _ = env.step(act)\n",
    "\n",
    "            # save action, reward\n",
    "            batch_acts.append(act)\n",
    "            ep_rews.append(rew)\n",
    "\n",
    "            if done:\n",
    "                # if episode is over, record info about episode\n",
    "                ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "                batch_rets.append(ep_ret)\n",
    "                batch_lens.append(ep_len)\n",
    "\n",
    "                # the weight for each logprob(a|s) is R(tau)\n",
    "                batch_weights += [ep_ret] * ep_len\n",
    "\n",
    "                # reset episode-specific variables\n",
    "                obs, done, ep_rews = env.reset(), False, []\n",
    "\n",
    "                # won't render again this epoch\n",
    "                finished_rendering_this_epoch = True\n",
    "\n",
    "                # end experience loop if we have enough of it\n",
    "                if len(batch_obs) > batch_size:\n",
    "                    break\n",
    "\n",
    "        # take a single policy gradient update step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
    "                                  act=torch.as_tensor(batch_acts, dtype=torch.int32),\n",
    "                                  weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n",
    "                                  )\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        return batch_loss, batch_rets, batch_lens\n",
    "\n",
    "    # training loop\n",
    "    for i in range(epochs):\n",
    "        batch_loss, batch_rets, batch_lens = train_one_epoch()\n",
    "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "                (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carter/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 \t loss: 17.483 \t return: 20.279 \t ep_len: 20.279\n",
      "epoch:   1 \t loss: 22.605 \t return: 23.933 \t ep_len: 23.933\n",
      "epoch:   2 \t loss: 22.555 \t return: 25.677 \t ep_len: 25.677\n",
      "epoch:   3 \t loss: 28.089 \t return: 29.857 \t ep_len: 29.857\n",
      "epoch:   4 \t loss: 31.012 \t return: 34.986 \t ep_len: 34.986\n",
      "epoch:   5 \t loss: 32.365 \t return: 35.610 \t ep_len: 35.610\n",
      "epoch:   6 \t loss: 34.119 \t return: 38.462 \t ep_len: 38.462\n",
      "epoch:   7 \t loss: 40.085 \t return: 44.416 \t ep_len: 44.416\n",
      "epoch:   8 \t loss: 37.368 \t return: 47.667 \t ep_len: 47.667\n",
      "epoch:   9 \t loss: 38.159 \t return: 47.657 \t ep_len: 47.657\n",
      "epoch:  10 \t loss: 46.638 \t return: 59.631 \t ep_len: 59.631\n",
      "epoch:  11 \t loss: 44.030 \t return: 57.364 \t ep_len: 57.364\n",
      "epoch:  12 \t loss: 45.956 \t return: 60.452 \t ep_len: 60.452\n",
      "epoch:  13 \t loss: 42.201 \t return: 61.108 \t ep_len: 61.108\n",
      "epoch:  14 \t loss: 49.790 \t return: 66.447 \t ep_len: 66.447\n",
      "epoch:  15 \t loss: 54.297 \t return: 69.932 \t ep_len: 69.932\n",
      "epoch:  16 \t loss: 50.655 \t return: 71.000 \t ep_len: 71.000\n",
      "epoch:  17 \t loss: 54.908 \t return: 72.725 \t ep_len: 72.725\n",
      "epoch:  18 \t loss: 63.195 \t return: 92.327 \t ep_len: 92.327\n",
      "epoch:  19 \t loss: 54.754 \t return: 81.935 \t ep_len: 81.935\n",
      "epoch:  20 \t loss: 57.965 \t return: 88.241 \t ep_len: 88.241\n",
      "epoch:  21 \t loss: 71.604 \t return: 98.373 \t ep_len: 98.373\n",
      "epoch:  22 \t loss: 81.170 \t return: 123.476 \t ep_len: 123.476\n",
      "epoch:  23 \t loss: 81.253 \t return: 122.000 \t ep_len: 122.000\n",
      "epoch:  24 \t loss: 78.078 \t return: 117.884 \t ep_len: 117.884\n",
      "epoch:  25 \t loss: 85.056 \t return: 135.351 \t ep_len: 135.351\n",
      "epoch:  26 \t loss: 87.316 \t return: 139.778 \t ep_len: 139.778\n",
      "epoch:  27 \t loss: 84.747 \t return: 136.579 \t ep_len: 136.579\n",
      "epoch:  28 \t loss: 91.203 \t return: 151.294 \t ep_len: 151.294\n",
      "epoch:  29 \t loss: 91.567 \t return: 156.000 \t ep_len: 156.000\n",
      "epoch:  30 \t loss: 96.820 \t return: 169.000 \t ep_len: 169.000\n",
      "epoch:  31 \t loss: 89.455 \t return: 158.344 \t ep_len: 158.344\n",
      "epoch:  32 \t loss: 92.193 \t return: 158.719 \t ep_len: 158.719\n",
      "epoch:  33 \t loss: 90.913 \t return: 157.969 \t ep_len: 157.969\n",
      "epoch:  34 \t loss: 82.946 \t return: 139.917 \t ep_len: 139.917\n",
      "epoch:  35 \t loss: 84.402 \t return: 143.229 \t ep_len: 143.229\n",
      "epoch:  36 \t loss: 82.829 \t return: 143.571 \t ep_len: 143.571\n",
      "epoch:  37 \t loss: 82.833 \t return: 144.429 \t ep_len: 144.429\n",
      "epoch:  38 \t loss: 88.171 \t return: 153.879 \t ep_len: 153.879\n",
      "epoch:  39 \t loss: 95.293 \t return: 174.172 \t ep_len: 174.172\n",
      "epoch:  40 \t loss: 98.963 \t return: 188.963 \t ep_len: 188.963\n",
      "epoch:  41 \t loss: 101.962 \t return: 191.037 \t ep_len: 191.037\n",
      "epoch:  42 \t loss: 101.778 \t return: 198.231 \t ep_len: 198.231\n",
      "epoch:  43 \t loss: 101.550 \t return: 196.346 \t ep_len: 196.346\n",
      "epoch:  44 \t loss: 98.954 \t return: 187.370 \t ep_len: 187.370\n",
      "epoch:  45 \t loss: 98.369 \t return: 187.333 \t ep_len: 187.333\n",
      "epoch:  46 \t loss: 96.418 \t return: 185.143 \t ep_len: 185.143\n",
      "epoch:  47 \t loss: 96.282 \t return: 182.000 \t ep_len: 182.000\n",
      "epoch:  48 \t loss: 90.901 \t return: 173.310 \t ep_len: 173.310\n",
      "epoch:  49 \t loss: 84.116 \t return: 156.000 \t ep_len: 156.000\n"
     ]
    }
   ],
   "source": [
    "train(env_name='CartPole-v0', render=False, lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Grad Log Prob Lemma\n",
    "$$\n",
    "E[\\nabla \\log P_\\theta(x) ] = 0\n",
    "$$\n",
    "[Proof](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#expected-grad-log-prob-lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward to go\n",
    "Note that in our original formualtion, the gradient is adjusted in accordance with all rewards which are obtained. Reality might dictate that instead we focus only on rewards taken an action not before, as the action was of no consequence for those awards. Call this the Reward to Go policy, the expression might be\n",
    "\n",
    "$$\\nabla J(\\pi_\\theta) = E\\left[ \\sum_{t=0}^T \\log \\pi(a_t ~|~ s_t) \\sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1}) \\right]$$\n",
    "\n",
    "(note that most of this - including the feedforward architecture - is the same as our above simple policy gradient method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_to_go(rews):\n",
    "    n = len(rews)\n",
    "    rtgs = np.zeros_like(rews)\n",
    "    for i in reversed(range(n)):\n",
    "        rtgs[i] = rews[i] + (rtgs[i+1] if i+1 < n else 0)\n",
    "    return rtgs\n",
    "\n",
    "def train(env_name='CartPole-v0', hidden_sizes=[32], lr=1e-2, \n",
    "          epochs=50, batch_size=5000, render=False):\n",
    "\n",
    "    # make environment, check spaces, get obs / act dims\n",
    "    env = gym.make(env_name)\n",
    "    assert isinstance(env.observation_space, Box), \\\n",
    "        \"This example only works for envs with continuous state spaces.\"\n",
    "    assert isinstance(env.action_space, Discrete), \\\n",
    "        \"This example only works for envs with discrete action spaces.\"\n",
    "\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    n_acts = env.action_space.n\n",
    "\n",
    "    # make core of policy network\n",
    "    logits_net = mlp(sizes=[obs_dim]+hidden_sizes+[n_acts])\n",
    "\n",
    "    # make function to compute action distribution\n",
    "    def get_policy(obs):\n",
    "        logits = logits_net(obs)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    # make action selection function (outputs int actions, sampled from policy)\n",
    "    def get_action(obs):\n",
    "        return get_policy(obs).sample().item()\n",
    "\n",
    "    # make loss function whose gradient, for the right data, is policy gradient\n",
    "    def compute_loss(obs, act, weights):\n",
    "        logp = get_policy(obs).log_prob(act)\n",
    "        return -(logp * weights).mean()\n",
    "\n",
    "    # make optimizer\n",
    "    optimizer = Adam(logits_net.parameters(), lr=lr)\n",
    "\n",
    "    # for training policy\n",
    "    def train_one_epoch():\n",
    "        # make some empty lists for logging.\n",
    "        batch_obs = []          # for observations\n",
    "        batch_acts = []         # for actions\n",
    "        batch_weights = []      # for reward-to-go weighting in policy gradient\n",
    "        batch_rets = []         # for measuring episode returns\n",
    "        batch_lens = []         # for measuring episode lengths\n",
    "\n",
    "        # reset episode-specific variables\n",
    "        obs = env.reset()       # first obs comes from starting distribution\n",
    "        done = False            # signal from environment that episode is over\n",
    "        ep_rews = []            # list for rewards accrued throughout ep\n",
    "\n",
    "        # render first episode of each epoch\n",
    "        finished_rendering_this_epoch = False\n",
    "\n",
    "        # collect experience by acting in the environment with current policy\n",
    "        while True:\n",
    "\n",
    "            # rendering\n",
    "            if (not finished_rendering_this_epoch) and render:\n",
    "                env.render()\n",
    "\n",
    "            # save obs\n",
    "            batch_obs.append(obs.copy())\n",
    "\n",
    "            # act in the environment\n",
    "            act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "            obs, rew, done, _ = env.step(act)\n",
    "\n",
    "            # save action, reward\n",
    "            batch_acts.append(act)\n",
    "            ep_rews.append(rew)\n",
    "\n",
    "            if done:\n",
    "                # if episode is over, record info about episode\n",
    "                ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "                batch_rets.append(ep_ret)\n",
    "                batch_lens.append(ep_len)\n",
    "\n",
    "                # the weight for each logprob(a_t|s_t) is reward-to-go from t\n",
    "                batch_weights += list(reward_to_go(ep_rews))\n",
    "\n",
    "                # reset episode-specific variables\n",
    "                obs, done, ep_rews = env.reset(), False, []\n",
    "\n",
    "                # won't render again this epoch\n",
    "                finished_rendering_this_epoch = True\n",
    "\n",
    "                # end experience loop if we have enough of it\n",
    "                if len(batch_obs) > batch_size:\n",
    "                    break\n",
    "\n",
    "        # take a single policy gradient update step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
    "                                  act=torch.as_tensor(batch_acts, dtype=torch.int32),\n",
    "                                  weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n",
    "                                  )\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        return batch_loss, batch_rets, batch_lens\n",
    "\n",
    "    # training loop\n",
    "    for i in range(epochs):\n",
    "        batch_loss, batch_rets, batch_lens = train_one_epoch()\n",
    "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "                (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 \t loss: 10.911 \t return: 23.758 \t ep_len: 23.758\n",
      "epoch:   1 \t loss: 10.999 \t return: 25.293 \t ep_len: 25.293\n",
      "epoch:   2 \t loss: 12.845 \t return: 27.762 \t ep_len: 27.762\n",
      "epoch:   3 \t loss: 14.065 \t return: 31.880 \t ep_len: 31.880\n",
      "epoch:   4 \t loss: 16.821 \t return: 34.979 \t ep_len: 34.979\n",
      "epoch:   5 \t loss: 19.236 \t return: 40.589 \t ep_len: 40.589\n",
      "epoch:   6 \t loss: 16.043 \t return: 36.022 \t ep_len: 36.022\n",
      "epoch:   7 \t loss: 21.132 \t return: 49.069 \t ep_len: 49.069\n",
      "epoch:   8 \t loss: 20.956 \t return: 50.616 \t ep_len: 50.616\n",
      "epoch:   9 \t loss: 22.223 \t return: 52.811 \t ep_len: 52.811\n",
      "epoch:  10 \t loss: 24.308 \t return: 58.918 \t ep_len: 58.918\n",
      "epoch:  11 \t loss: 22.412 \t return: 61.427 \t ep_len: 61.427\n",
      "epoch:  12 \t loss: 23.503 \t return: 63.430 \t ep_len: 63.430\n",
      "epoch:  13 \t loss: 29.038 \t return: 73.145 \t ep_len: 73.145\n",
      "epoch:  14 \t loss: 27.452 \t return: 71.648 \t ep_len: 71.648\n",
      "epoch:  15 \t loss: 33.064 \t return: 90.018 \t ep_len: 90.018\n",
      "epoch:  16 \t loss: 34.274 \t return: 91.909 \t ep_len: 91.909\n",
      "epoch:  17 \t loss: 36.313 \t return: 101.060 \t ep_len: 101.060\n",
      "epoch:  18 \t loss: 41.134 \t return: 123.902 \t ep_len: 123.902\n",
      "epoch:  19 \t loss: 42.773 \t return: 127.200 \t ep_len: 127.200\n",
      "epoch:  20 \t loss: 46.106 \t return: 146.686 \t ep_len: 146.686\n",
      "epoch:  21 \t loss: 43.397 \t return: 140.417 \t ep_len: 140.417\n",
      "epoch:  22 \t loss: 47.379 \t return: 153.061 \t ep_len: 153.061\n",
      "epoch:  23 \t loss: 48.380 \t return: 160.781 \t ep_len: 160.781\n",
      "epoch:  24 \t loss: 52.093 \t return: 180.643 \t ep_len: 180.643\n",
      "epoch:  25 \t loss: 53.253 \t return: 186.222 \t ep_len: 186.222\n",
      "epoch:  26 \t loss: 53.135 \t return: 185.259 \t ep_len: 185.259\n",
      "epoch:  27 \t loss: 53.837 \t return: 189.333 \t ep_len: 189.333\n",
      "epoch:  28 \t loss: 50.575 \t return: 179.821 \t ep_len: 179.821\n",
      "epoch:  29 \t loss: 51.220 \t return: 187.333 \t ep_len: 187.333\n",
      "epoch:  30 \t loss: 47.784 \t return: 169.867 \t ep_len: 169.867\n",
      "epoch:  31 \t loss: 49.004 \t return: 173.793 \t ep_len: 173.793\n",
      "epoch:  32 \t loss: 48.584 \t return: 175.966 \t ep_len: 175.966\n",
      "epoch:  33 \t loss: 49.626 \t return: 185.778 \t ep_len: 185.778\n",
      "epoch:  34 \t loss: 50.140 \t return: 188.074 \t ep_len: 188.074\n",
      "epoch:  35 \t loss: 50.315 \t return: 188.444 \t ep_len: 188.444\n",
      "epoch:  36 \t loss: 52.045 \t return: 196.423 \t ep_len: 196.423\n",
      "epoch:  37 \t loss: 51.200 \t return: 194.423 \t ep_len: 194.423\n",
      "epoch:  38 \t loss: 51.504 \t return: 196.423 \t ep_len: 196.423\n",
      "epoch:  39 \t loss: 51.930 \t return: 199.462 \t ep_len: 199.462\n",
      "epoch:  40 \t loss: 51.354 \t return: 198.692 \t ep_len: 198.692\n",
      "epoch:  41 \t loss: 51.718 \t return: 198.808 \t ep_len: 198.808\n",
      "epoch:  42 \t loss: 51.494 \t return: 198.385 \t ep_len: 198.385\n",
      "epoch:  43 \t loss: 51.626 \t return: 198.538 \t ep_len: 198.538\n",
      "epoch:  44 \t loss: 51.205 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  45 \t loss: 51.298 \t return: 198.038 \t ep_len: 198.038\n",
      "epoch:  46 \t loss: 52.128 \t return: 199.231 \t ep_len: 199.231\n",
      "epoch:  47 \t loss: 51.481 \t return: 196.000 \t ep_len: 196.000\n",
      "epoch:  48 \t loss: 51.205 \t return: 199.269 \t ep_len: 199.269\n",
      "epoch:  49 \t loss: 51.346 \t return: 198.115 \t ep_len: 198.115\n"
     ]
    }
   ],
   "source": [
    "train(env_name='CartPole-v0', render=False, lr=1e-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
