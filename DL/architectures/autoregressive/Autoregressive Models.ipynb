{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are we trying to do?\n",
    "Autoregressive models estimate a distribution $p_\\theta(x)$ drawing from samples $x^{(1)}, \\ldots, x^{(n)}$.The name Autoregessive comes from estimating $x_i$, using $x_{<i}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Chain Rule \n",
    "Take any old distribution\n",
    "$$p(x_1, \\ldots, x_n)$$\n",
    "The chain rule states\n",
    "$$ p(x_1, \\ldots, x_n) = \\prod_{i=1}^n p(x_i ~|~ x_1, \\ldots, x_{i-1}) = \\prod_{i=1}^n p(x_i ~|~ x_{< i})$$\n",
    "This will be important when talking about how many parameters it would take to hold all possible values in a joint distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Curse of Dimensionality** \n",
    "Fitting distributions is all about learning some distribution to best represent/ fit our collection of data points. One of the issues with fitting higher dimensional distributions is representation. For example compare holding the distribution of a single bernoulli random variable in memory\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "p(x_1 = 1) & p(x_1 = 0)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "To holding the joint distribution of 2 bernoulli random variables\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "p(x_2 = 1 | x_1 = 1) & p(x_2 = 1 | x_1 = 1) \n",
    "& p(x_2 = 0 | x_1 = 1) & p(x_2 = 0 | x_1 = 0)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "We see already that the memory complexity has doubled. And with 3 bernoulli random variables\n",
    "$$\\begin{pmatrix} p(x_3=0 | x_2=0, x_1=0) & p(x_3=0 | x_2=0, x_1=1) & p(x_3=0 | x_2=1, x_1=0) & p(x_3=0 | x_2=1, x_1=1)  & \\ldots \\end{pmatrix}$$\n",
    "In this simple case with Benoulli random variables, adding another random variable triples the number of parameters required\n",
    "\n",
    "\n",
    "Imagine then we are trying to generate a simple 28 by 28 black and white image. The memory complexity would be\n",
    "$$2^{784} \\approx 10^{236}$$\n",
    "\n",
    "Also imagine this from a modeling perspective!\n",
    "- We could never have all the possible training examples!\n",
    "- Each image has their own parameter, no generalization or extrapolation what so ever!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum Likelihood Estimate\n",
    "The goal for MLE is to minimize the negative log likelihood (NLL) for each training sample $x^{(i)}$\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n \\text{NLL}({p_\\theta(x^{(i)})}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Log Likelihood Definition\n",
    "If $p_\\theta(x^{(i)}) = \\hat{x}$, and $\\hat{x}$ represents the output, and $x$ represents the target, then \n",
    "$$\\text{NLL}(p_\\theta(x^{(i)})) = \\text{NLL}(\\hat{x}) = x \\cdot \\log(\\hat{x}) + (1 - x) \\cdot \\log(1- \\hat{x})$$\n",
    "\n",
    "[Why do I need to add log to my nll in pytorch?](https://discuss.pytorch.org/t/why-there-is-no-log-operator-in-implementation-of-torch-nn-nllloss/16610/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2231), 0.2231435513142097)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_hat = torch.tensor([[.2, .8]])\n",
    "x = torch.tensor([1])\n",
    "\n",
    "F.nll_loss(torch.log(x_hat), x), -1 *(0 * math.log(.2) + 1 * math.log(.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy\n",
    "One thing we will see is that maximizing the NLL is the same as minimizing the Cross Entropy\n",
    "$$ \\sum x \\cdot \\log(\\hat{x}) = \\text{Cross Entropy}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4375), tensor(0.4375))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(x_hat, x), F.nll_loss(F.log_softmax(x_hat, dim=1), x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
