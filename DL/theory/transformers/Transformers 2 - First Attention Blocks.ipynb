{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "atlantic-president",
   "metadata": {},
   "source": [
    "# Attention\n",
    "In **Transformers 1 - Before Attention** 2 LSTMs were used to carry the cell state and hidden state between the encoder and decoder\n",
    "$$dc_{1} = f \\odot ec_{t} + i \\odot g$$\n",
    "$$dh_{1} = o \\odot \\tanh(ec_t)$$\n",
    "\n",
    "Attention presents the following modification\n",
    "$$ ah_{t} = \\sum_{i=1}^t a_i \\cdot dh_{i} $$\n",
    "\n",
    "Now passing the Attention Layer output to the decoder, the question asked is what do we do with the cell state? \n",
    "\n",
    "Do we pass both the cell state and the hidden state scaled to the decoder lstm? \n",
    "\n",
    "Just the cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Lib\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tokenization\n",
    "import spacy \n",
    "\n",
    "# Loading Bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Torch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Dataloader Custom Module\n",
    "from sample_dataloader import get_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-marina",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = os.path.join(Path(os.getcwd()).parent.parent.parent, \"Datasets/\")\n",
    "gpu = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-speaking",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-faculty",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-automation",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, validset, testset, de_vocab, en_vocab = get_dataloaders(batch_size=128, device=gpu, data_root=data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-breed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all these indices are the same for french and english\n",
    "PAD_IDX = de_vocab['<pad>']\n",
    "SOS_IDX = de_vocab['<sos>']\n",
    "EOS_IDX = de_vocab['<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-suite",
   "metadata": {},
   "source": [
    "## Implementation Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, enc_hid_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        \n",
    "        # hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        # outputs are always from the last layer\n",
    "        \n",
    "        # hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        # hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        # initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        #  encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        # outputs = [src len, batch size, enc hid dim * 2]\n",
    "        # hidden = [batch size, dec hid dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        attention = self.v(energy).squeeze(2)\n",
    "                \n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        # this are the alignment scores for each of the encoder\n",
    "        # outputs (note that the encoder outputs are the hidden states\n",
    "        # of all previous encodings)\n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "        \n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        # batch matrix multiplication\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "                    \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        \n",
    "        return prediction, hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\" this is the same as in the previous module \"\"\"\n",
    "    def __init__(self, encoder, decoder, device, trg_vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.trg_vocab_size = trg_vocab_size \n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=.5):\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        \n",
    "        outputs = torch.zeros(trg_len, batch_size, self.trg_vocab_size).to(self.device)\n",
    "        \n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        \n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            \n",
    "            outputs[t] = output\n",
    "            \n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            top1 = output.argmax(1) \n",
    "    \n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-ecuador",
   "metadata": {},
   "source": [
    "### Initialization + Number of Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-clarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(de_vocab)\n",
    "OUTPUT_DIM = len(en_vocab)\n",
    "ENC_EMB_DIM = 128\n",
    "DEC_EMB_DIM = 128\n",
    "ENC_HID_DIM = 256\n",
    "DEC_HID_DIM = 256\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, gpu, OUTPUT_DIM).to(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model weights\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-communist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-boundary",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip=1, num_epochs=10):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for _ in range(num_epochs):\n",
    "        for i, batch in tqdm(enumerate(iterator), desc=\"iteration\"):\n",
    "            src = batch.src\n",
    "            trg = batch.trg \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(src, trg)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            # clip the gradients to prevent them from exploding (a common issue in RNNs)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            optimizer.step()\n",
    "                    \n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-incidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "model, losses = train(model, trainset, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-accuracy",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-advertiser",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=list(range(len(losses))), y=losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-employment",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_2_str(tensor, vocab=de_vocab.itos):\n",
    "    return \" \".join([vocab[int(token)] for token in tensor if vocab[int(token)] not in ['<eos>', '<pad>', '.']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sample = next(iter(trainset))\n",
    "    src, trg = sample.src, sample.trg\n",
    "    output = model(src, trg)\n",
    "    output_tensor = output.argmax(2)[:, 0]\n",
    "    target_tensor = trg[:, 0]\n",
    "    \n",
    "    output = tensor_2_str(output_tensor, en_vocab.itos)\n",
    "    expected = tensor_2_str(target_tensor, en_vocab.itos)\n",
    "    N = max(len(output), len(expected)) + len(\"Expected: \")\n",
    "    \n",
    "    print(\"=\"*N)\n",
    "    print(\"Output: {}\".format(output).center(N))\n",
    "    print(\"=\"*N)\n",
    "    \n",
    "    print(\"=\"*N)\n",
    "    print(\"Expected: {}\".format(expected).center(N))\n",
    "    print(\"=\"*N)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
