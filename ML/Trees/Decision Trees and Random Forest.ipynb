{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "##### References\n",
    "[Decision Trees from Scratch](https://medium.com/@penggongting/implementing-decision-tree-from-scratch-in-python-c732e7c69aea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "from itertools import starmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy Formulation\n",
    "Part of building a decision tree deciding how the tree branches and breaks off into individual leaves. **Entropy** is how we characterize if a split is good or not. \n",
    "$$Entropy = -\\sum_X P(X = Class) * \\log_2 P(X = Class)$$\n",
    "If a split is good its got very low Entropy, meaning most of the data on either side of the split is the same class. Roughly speaking that algorithm looks something like this\n",
    "\n",
    "```\n",
    "Divide all the data on a node into 2 parts, left and right\n",
    "> Compute the Entropy of Left and Right\n",
    "> return len(Left)/len(Total) * Entropy Left + len(Right)/len(Right) * Entropy Right\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _best_split(features, data):\n",
    "    split = starmap(_split, zip(features, data))\n",
    "    split = sorted(list(split), key= lambda x : x[1]) # sort by entropy\n",
    "    return split[0] # return feature with the lowest entropy\n",
    "\n",
    "def _split(feature, data):\n",
    "    '''\n",
    "    feature - the thing we are splitting on\n",
    "    data - the thing which we are breaking up using that feature\n",
    "    '''\n",
    "    best_entropy, best_val = 1e6, None\n",
    "    for val in feature:\n",
    "        sel = val < data\n",
    "        left, right = data[sel], data[~sel]\n",
    "        curr_entropy = _entropy(left, right)\n",
    "\n",
    "        if curr_entropy < best_entropy:\n",
    "            best_entropy = curr_entropy\n",
    "            best_val = val\n",
    "    return feature, best_entropy, best_val\n",
    "\n",
    "def _get_entropy(side):\n",
    "    '''\n",
    "    Compute the entropy for a given side\n",
    "    '''\n",
    "    data_counts = Counter(side)\n",
    "    n = len(side)\n",
    "    total = 0\n",
    "    for c in data_counts.keys():\n",
    "        prob = data_counts[c]/n\n",
    "        prob *= math.log2(prob)\n",
    "        total += prob\n",
    "    return total\n",
    "\n",
    "def _entropy(left, right):\n",
    "    '''\n",
    "    Every split is characterized by breaking some dataset into 2 sides\n",
    "    the right side and the left side (left, right)\n",
    "\n",
    "    The net entropy is thus the combined entropy of those 2 sides\n",
    "    '''\n",
    "    left_entropy, right_entropy = _get_entropy(left), _get_entropy(right)\n",
    "    n = len(left) + len(right)\n",
    "    return len(left)/n * left_entropy + len(right)/n * right_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, max_depth):\n",
    "        self.current_depth = 0\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "    \n",
    "    def fit(self, X, y, feature_names, parent={}, depth=0):\n",
    "        if parent is None:\n",
    "            return None\n",
    "        elif len(y) == 0:\n",
    "            return None\n",
    "        elif all([_tmp == y[0] for _tmp in y]):\n",
    "            return {'val':y[0]}\n",
    "        elif depth >= self.max_depth: \n",
    "            return None\n",
    "        \n",
    "        # recursively generate trees\n",
    "        else:\n",
    "            feature, best_entropy, cutoff = _best_split(X, y)\n",
    "            \n",
    "            # cut data into its components\n",
    "            left_sel = X[: , feature] < cutoff\n",
    "            right_sel = X[: , feature] >= cutoff\n",
    "            left_X, left_y = X[left_sel], y[left_sel]\n",
    "            right_X, right_y = X[right_sel], y[right_sel]\n",
    "            \n",
    "            parent = {\n",
    "                'feature' : feature,\n",
    "                'cutoff' : cutoff,\n",
    "            }\n",
    "            parent['left'] = self.fit(X, y, feature_names, {}, depth+1)\n",
    "            parent['right'] = self.fit(X, y, feature_names, {}, depth+1)\n",
    "            self.depth += 1\n",
    "            self.tree = parent\n",
    "            return parent # when we recurse all the way back up this will be the root\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X, y, feature_names = data['data'], data['target'], data['feature_names']\n",
    "assert type(X) == np.ndarray and type(y) == np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dT = DecisionTree(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-073e468e4516>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-5f39d6e48998>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, feature_names, parent, depth)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m# cut data into its components\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mleft_sel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mright_sel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mleft_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft_sel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft_sel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "tree = dT.fit(X, y, feature_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
